{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd07aea",
   "metadata": {},
   "source": [
    "# ğŸš€ Model 1. XGBoost + Optuna Tuning\n",
    "\n",
    "íŠ¸ë¦¬ ê¸°ë°˜ ë¶€ìŠ¤íŒ… ëª¨ë¸ì˜ ì •ì„ì¸ **XGBoost**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "ë‹¨ìˆœ í•™ìŠµì´ ì•„ë‹ˆë¼ **Optuna**ë¥¼ í™œìš©í•˜ì—¬ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°(í•™ìŠµë¥ , ê¹Šì´ ë“±)ë¥¼ ìë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ“‹ ì§„í–‰ ìˆœì„œ\n",
    "1.  ë°ì´í„° ë¡œë“œ (`train_enriched.csv`, `test_enriched.csv`)\n",
    "2.  Optunaë¥¼ ì´ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (Hyperparameter Tuning)\n",
    "3.  ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ\n",
    "4.  ê²°ê³¼ ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± (`submission_xgb_optuna.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e49080d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:02:59,456] A new study created in memory with name: no-name-a98f9736-abc8-4011-a36c-563ddc186c8f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: (12348, 11)\n",
      "\n",
      "Optuna íŠœë‹ ì‹œì‘ (20íšŒ ì‹œë„)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-02 14:03:01,818] Trial 0 finished with value: 14.482791937255477 and parameters: {'n_estimators': 179, 'learning_rate': 0.16947298447891884, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.7178046042891915, 'colsample_bytree': 0.7909285413492447, 'reg_alpha': 9.202737634194198, 'reg_lambda': 3.2333572562267765}. Best is trial 0 with value: 14.482791937255477.\n",
      "[I 2025-12-02 14:03:04,705] Trial 1 finished with value: 14.247589721708668 and parameters: {'n_estimators': 701, 'learning_rate': 0.10348342727249248, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.9104459438814834, 'colsample_bytree': 0.9219632855112035, 'reg_alpha': 4.287898428769275, 'reg_lambda': 6.295557820568888}. Best is trial 1 with value: 14.247589721708668.\n",
      "[I 2025-12-02 14:03:06,874] Trial 2 finished with value: 14.521789612066586 and parameters: {'n_estimators': 423, 'learning_rate': 0.11172770383780894, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.7130841582114915, 'colsample_bytree': 0.7447981183850173, 'reg_alpha': 9.224483257436471, 'reg_lambda': 1.540055460832086}. Best is trial 1 with value: 14.247589721708668.\n",
      "[I 2025-12-02 14:03:13,406] Trial 3 finished with value: 15.051568691624464 and parameters: {'n_estimators': 531, 'learning_rate': 0.17506829601040294, 'max_depth': 11, 'min_child_weight': 6, 'subsample': 0.7803780821823495, 'colsample_bytree': 0.7259518931944298, 'reg_alpha': 9.973689888983092, 'reg_lambda': 0.9606564643206972}. Best is trial 1 with value: 14.247589721708668.\n",
      "[I 2025-12-02 14:03:17,452] Trial 4 finished with value: 14.348497853064876 and parameters: {'n_estimators': 662, 'learning_rate': 0.10116381163135155, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.9388180260421384, 'colsample_bytree': 0.9887630676078685, 'reg_alpha': 7.072453974255418, 'reg_lambda': 9.36613412910317}. Best is trial 1 with value: 14.247589721708668.\n",
      "[I 2025-12-02 14:03:18,637] Trial 5 finished with value: 13.57308811165819 and parameters: {'n_estimators': 574, 'learning_rate': 0.0322056674263037, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.7566579637856901, 'colsample_bytree': 0.8519380710469422, 'reg_alpha': 6.21100030636547, 'reg_lambda': 5.881608544333102}. Best is trial 5 with value: 13.57308811165819.\n",
      "[I 2025-12-02 14:03:24,014] Trial 6 finished with value: 15.47277859379202 and parameters: {'n_estimators': 703, 'learning_rate': 0.1910928808966633, 'max_depth': 8, 'min_child_weight': 6, 'subsample': 0.606310696125159, 'colsample_bytree': 0.9810500998956339, 'reg_alpha': 4.916734822589327, 'reg_lambda': 2.7631118411492994}. Best is trial 5 with value: 13.57308811165819.\n",
      "[I 2025-12-02 14:03:29,646] Trial 7 finished with value: 14.709238840098106 and parameters: {'n_estimators': 306, 'learning_rate': 0.19861698424383162, 'max_depth': 12, 'min_child_weight': 4, 'subsample': 0.9101023001067818, 'colsample_bytree': 0.8190990146840655, 'reg_alpha': 7.178659453747491, 'reg_lambda': 9.047892202306059}. Best is trial 5 with value: 13.57308811165819.\n",
      "[I 2025-12-02 14:03:32,113] Trial 8 finished with value: 14.458646607766415 and parameters: {'n_estimators': 285, 'learning_rate': 0.08189142873482469, 'max_depth': 9, 'min_child_weight': 6, 'subsample': 0.6023002538510764, 'colsample_bytree': 0.6579652942264579, 'reg_alpha': 6.135795545082904, 'reg_lambda': 0.9797570509572173}. Best is trial 5 with value: 13.57308811165819.\n",
      "[I 2025-12-02 14:03:33,803] Trial 9 finished with value: 14.266866064321079 and parameters: {'n_estimators': 680, 'learning_rate': 0.18982767112968618, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.7956363593732118, 'colsample_bytree': 0.6621481851425408, 'reg_alpha': 1.7944407374336024, 'reg_lambda': 7.055034180159402}. Best is trial 5 with value: 13.57308811165819.\n",
      "[I 2025-12-02 14:03:35,678] Trial 10 finished with value: 13.558415734906855 and parameters: {'n_estimators': 994, 'learning_rate': 0.01206604083229787, 'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.842084333213211, 'colsample_bytree': 0.8746672714130976, 'reg_alpha': 0.41057568072120443, 'reg_lambda': 4.953373926974518}. Best is trial 10 with value: 13.558415734906855.\n",
      "[I 2025-12-02 14:03:37,761] Trial 11 finished with value: 13.562191096555486 and parameters: {'n_estimators': 996, 'learning_rate': 0.01621704382346919, 'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.8347343360083574, 'colsample_bytree': 0.8730777889657987, 'reg_alpha': 0.36612094306873955, 'reg_lambda': 4.876848192055102}. Best is trial 10 with value: 13.558415734906855.\n",
      "[I 2025-12-02 14:03:40,725] Trial 12 finished with value: 15.392981955948375 and parameters: {'n_estimators': 976, 'learning_rate': 0.2794876825833855, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.8536937464351594, 'colsample_bytree': 0.8843325995558546, 'reg_alpha': 0.21144956361626477, 'reg_lambda': 4.087267945768062}. Best is trial 10 with value: 13.558415734906855.\n",
      "[I 2025-12-02 14:03:42,701] Trial 13 finished with value: 13.55764057518374 and parameters: {'n_estimators': 994, 'learning_rate': 0.020291198843173975, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.8519959150948871, 'colsample_bytree': 0.9257134287001818, 'reg_alpha': 2.4797831690932113, 'reg_lambda': 4.72492571198371}. Best is trial 13 with value: 13.55764057518374.\n",
      "[I 2025-12-02 14:03:45,249] Trial 14 finished with value: 13.8238810759777 and parameters: {'n_estimators': 837, 'learning_rate': 0.052423707799331296, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9782565605658657, 'colsample_bytree': 0.92901916532655, 'reg_alpha': 2.683831344936974, 'reg_lambda': 7.135602311724477}. Best is trial 13 with value: 13.55764057518374.\n",
      "[I 2025-12-02 14:03:46,839] Trial 15 finished with value: 13.631474797858342 and parameters: {'n_estimators': 863, 'learning_rate': 0.061673540058572424, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.8555636531392521, 'colsample_bytree': 0.9320683395619889, 'reg_alpha': 2.685973917424283, 'reg_lambda': 4.695624190495692}. Best is trial 13 with value: 13.55764057518374.\n",
      "[I 2025-12-02 14:03:49,457] Trial 16 finished with value: 13.532636366413191 and parameters: {'n_estimators': 849, 'learning_rate': 0.013625412480311919, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.8804309946238006, 'colsample_bytree': 0.7937930843849573, 'reg_alpha': 1.6942488450715314, 'reg_lambda': 2.997386705805371}. Best is trial 16 with value: 13.532636366413191.\n",
      "[I 2025-12-02 14:03:53,023] Trial 17 finished with value: 14.46403218855643 and parameters: {'n_estimators': 873, 'learning_rate': 0.13074865232668628, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8893014098582627, 'colsample_bytree': 0.7668928053393599, 'reg_alpha': 3.5486312462026346, 'reg_lambda': 2.3690172808112875}. Best is trial 16 with value: 13.532636366413191.\n",
      "[I 2025-12-02 14:03:54,745] Trial 18 finished with value: 14.70176448077035 and parameters: {'n_estimators': 798, 'learning_rate': 0.23786031534384888, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.9719108649452862, 'colsample_bytree': 0.6135097777212357, 'reg_alpha': 1.4361260856537896, 'reg_lambda': 0.03554406711829117}. Best is trial 16 with value: 13.532636366413191.\n",
      "[I 2025-12-02 14:03:58,320] Trial 19 finished with value: 14.21453415484179 and parameters: {'n_estimators': 901, 'learning_rate': 0.05873578234154503, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.6655897286406322, 'colsample_bytree': 0.8116745216573443, 'reg_alpha': 1.7007671074344977, 'reg_lambda': 3.825820738804035}. Best is trial 16 with value: 13.532636366413191.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Trial: 13.5326\n",
      "Best Params: {'n_estimators': 849, 'learning_rate': 0.013625412480311919, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.8804309946238006, 'colsample_bytree': 0.7937930843849573, 'reg_alpha': 1.6942488450715314, 'reg_lambda': 2.997386705805371}\n",
      "\n",
      "ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ ë°ì´í„° ì¬í•™ìŠµ ì¤‘...\n",
      "\n",
      "XGBoost íŠœë‹ ì™„ë£Œ! ê²°ê³¼ ì €ì¥ë¨: submission_xgb_optuna.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Model 1. XGBoost with Optuna Tuning\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1. ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„\n",
    "# ----------------------------------------------------------\n",
    "PREPROCESS_DIR = './data_preprocess'\n",
    "SUBMISSION_DIR = './open_track1'\n",
    "\n",
    "TRAIN_PATH = os.path.join(PREPROCESS_DIR, 'train_enriched.csv')\n",
    "TEST_PATH = os.path.join(PREPROCESS_DIR, 'test_enriched.csv')\n",
    "SUBMISSION_PATH = os.path.join(SUBMISSION_DIR, 'sample_submission.csv')\n",
    "\n",
    "print(\"ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# ì¸ì½”ë”© (Label Encoding)\n",
    "cat_features = ['type_name', 'prev_type_name']\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in cat_features:\n",
    "    all_values = pd.concat([train_df[col].astype(str), test_df[col].astype(str)]).unique()\n",
    "    le.fit(all_values)\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "\n",
    "# í”¼ì²˜ ì„ íƒ\n",
    "feature_cols = [\n",
    "    'start_x', 'start_y', 'type_name', 'team_id', 'time_seconds',\n",
    "    'prev_type_name', 'prev_end_x', 'prev_end_y',\n",
    "    'dist_to_goal', 'angle_to_goal', 'dist_to_center'\n",
    "]\n",
    "target_cols = ['end_x', 'end_y']\n",
    "\n",
    "X = train_df[feature_cols]\n",
    "y = train_df[target_cols]\n",
    "X_test = test_df[feature_cols]\n",
    "test_ids = test_df['game_episode']\n",
    "\n",
    "# íŠœë‹ìš© ë°ì´í„° ë¶„ë¦¬ (8:2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X_train.shape}\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. Optunaë¥¼ ì´ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "# ----------------------------------------------------------\n",
    "def objective(trial):\n",
    "    \"\"\"Optunaê°€ ìµœì í™”í•  ëª©í‘œ í•¨ìˆ˜ (RMSE ìµœì†Œí™”)\"\"\"\n",
    "    \n",
    "    # íŠœë‹í•  íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # XGBoostëŠ” ë‹¤ì¤‘ íƒ€ê²Ÿ(Multi-output)ì„ ê¸°ë³¸ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ,\n",
    "    # MultiOutputRegressorë¥¼ ì“°ê±°ë‚˜ ê°ê° í•™ìŠµí•´ì•¼ í•˜ì§€ë§Œ,\n",
    "    # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ Xì¶•ê³¼ Yì¶•ì— ëŒ€í•´ ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "    # (ì‹¤ì œë¡œëŠ” ê°ê° íŠœë‹í•˜ë©´ ë” ì¢‹ì§€ë§Œ ì‹œê°„ì´ 2ë°° ê±¸ë¦¼)\n",
    "    \n",
    "    # Xì¶• ì˜ˆì¸¡ ëª¨ë¸\n",
    "    model_x = xgb.XGBRegressor(**params)\n",
    "    model_x.fit(X_train, y_train['end_x'])\n",
    "    pred_x = model_x.predict(X_val)\n",
    "    \n",
    "    # Yì¶• ì˜ˆì¸¡ ëª¨ë¸\n",
    "    model_y = xgb.XGBRegressor(**params)\n",
    "    model_y.fit(X_train, y_train['end_y'])\n",
    "    pred_y = model_y.predict(X_val)\n",
    "    \n",
    "    # í‰ê·  RMSE ê³„ì‚°\n",
    "    rmse_x = np.sqrt(mean_squared_error(y_val['end_x'], pred_x))\n",
    "    rmse_y = np.sqrt(mean_squared_error(y_val['end_y'], pred_y))\n",
    "    \n",
    "    return (rmse_x + rmse_y) / 2\n",
    "\n",
    "print(\"\\nOptuna íŠœë‹ ì‹œì‘ (20íšŒ ì‹œë„)...\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20) # ì‹œê°„ ê´€ê³„ìƒ 20íšŒë§Œ (ë” ëŠ˜ë ¤ë„ ë¨)\n",
    "\n",
    "print(f\"\\nBest Trial: {study.best_trial.value:.4f}\")\n",
    "print(\"Best Params:\", study.best_params)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "# ----------------------------------------------------------\n",
    "print(\"\\nìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ ë°ì´í„° ì¬í•™ìŠµ ì¤‘...\")\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params['random_state'] = 42\n",
    "best_params['n_jobs'] = -1\n",
    "\n",
    "# ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ (X, y ì „ì²´ ì‚¬ìš©)\n",
    "final_model_x = xgb.XGBRegressor(**best_params)\n",
    "final_model_y = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "final_model_x.fit(X, y['end_x'])\n",
    "final_model_y.fit(X, y['end_y'])\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "pred_x = final_model_x.predict(X_test)\n",
    "pred_y = final_model_y.predict(X_test)\n",
    "\n",
    "# Clipping (ê²½ê¸°ì¥ ê·œê²© ë³´ì •)\n",
    "pred_x = np.clip(pred_x, 0, 105)\n",
    "pred_y = np.clip(pred_y, 0, 68)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4. ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "# ----------------------------------------------------------\n",
    "submission_df = pd.DataFrame({\n",
    "    'game_episode': test_ids,\n",
    "    'end_x': pred_x,\n",
    "    'end_y': pred_y\n",
    "})\n",
    "\n",
    "sample_submission = pd.read_csv(SUBMISSION_PATH)\n",
    "final_submission = pd.merge(sample_submission[['game_episode']], submission_df, on='game_episode', how='left')\n",
    "final_submission.fillna(50.0, inplace=True)\n",
    "\n",
    "save_path = 'submission_xgb_optuna.csv'\n",
    "final_submission.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"\\nXGBoost íŠœë‹ ì™„ë£Œ! ê²°ê³¼ ì €ì¥ë¨: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
